* Meta-data :noexport:
#+TITLE:
#+AUTHOR: Kazuki Yoshida
#+OPTIONS: toc:nil
#+OPTIONS: ^:{}
# LATEX configurations
#+LATEX_CLASS_OPTIONS: [dvipdfmx,10pt]
#+LATEX_HEADER: %% Margin
#+LATEX_HEADER: %% \usepackage[margin=1.5cm]{geometry}
#+LATEX_HEADER: \usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm, headsep=4pt]{geometry}
#+LATEX_HEADER: %% \addtolength{\topmargin}{0.3cm}
#+LATEX_HEADER: %% \addtolength{\textheight}{1.75in}
#+LATEX_HEADER: %% Math
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: %% Allow new page within align
#+LATEX_HEADER: \allowdisplaybreaks
#+LATEX_HEADER: \usepackage{cancel}
#+LATEX_HEADER: % % Code
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \lstset{basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single}
#+LATEX_HEADER: \usepackage[cache=false]{minted}
#+LATEX_HEADER: \usemintedstyle{vs}
#+LATEX_HEADER: %% Graphics
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{grffile}
#+LATEX_HEADER: %% DAG
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{positioning,shapes.geometric}
#+LATEX_HEADER: %% Date
#+LATEX_HEADER: \usepackage[yyyymmdd]{datetime}
#+LATEX_HEADER: \renewcommand{\dateseparator}{--}
#+LATEX_HEADER: %% Header
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \fancyhf{} % Erase first to supress section names
#+LATEX_HEADER: \fancyhead[L]{Kazuki Yoshida} % LEFT
#+LATEX_HEADER: \fancyhead[C]{} % CENTER
#+LATEX_HEADER: \fancyhead[R]{\today} % RIGHT
#+LATEX_HEADER: \fancyfoot[C]{\thepage}
#+LATEX_HEADER: %% \fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
#+LATEX_HEADER: %% Section font size
#+LATEX_HEADER: \usepackage{sectsty}
#+LATEX_HEADER: \sectionfont{\small}
#+LATEX_HEADER: \subsectionfont{\small}
#+LATEX_HEADER: \subsubsectionfont{\small}
#+LATEX_HEADER: %% Section numbering
#+LATEX_HEADER: %% http://tex.stackexchange.com/questions/3177/how-to-change-the-numbering-of-part-chapter-section-to-alphabetical-r
#+LATEX_HEADER: %% \renewcommand\thesection{\alph{section}}
#+LATEX_HEADER: %% \renewcommand\thesubsection{\thesection.\arabic{subsection}}
#+LATEX_HEADER: %% \renewcommand{\thesubsubsection}{\thesubsection.\alph{subsubsection}}
#+LATEX_HEADER: %%
#+LATEX_HEADER: %% http://tex.stackexchange.com/questions/40067/numbering-sections-with-sequential-integers
#+LATEX_HEADER: %% \usepackage{chngcntr}
#+LATEX_HEADER: %% \counterwithout{subsection}{section}
#+LATEX_HEADER: %% enumerate
#+LATEX_HEADER: \usepackage{enumerate}
#+LATEX_HEADER: %% double space
#+LATEX_HEADER: %% \usepackage{setspace}
#+LATEX_HEADER: %% \linespread{2}
#+LATEX_HEADER: %% Paragraph Indentation
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \setlength{\parindent}{0em}
#+LATEX_HEADER: %% Spacing after headings
#+LATEX_HEADER: %% http://tex.stackexchange.com/questions/53338/reducing-spacing-after-headings
#+LATEX_HEADER: \usepackage{titlesec}
#+LATEX_HEADER: \titlespacing      \section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
#+LATEX_HEADER: \titlespacing   \subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
#+LATEX_HEADER: \titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
#+LATEX_HEADER: %% Fix figures and tables by [H]
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: %% Allow URL embedding
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \input{\string~/.emacs.d/misc/GrandMacros}
# ############################################################################ #

* Opening

- This year limited to Harvard affiliates only
- stochastic methods and bayesian inference
  - stochastic gradient descent
- stochastic gradient descent


* Learning Models

- why model
  - prediction
  - explaining

- hypothesis space: potential functional forms
- overfitting: often what's wrong
- generating process is not a straight line.
- OLS was done
- Empirical risk minimization on the points that we have in the sample.
- Sample has to be representative of the population. random sampling.
- error on the entire population (out-of-sample error) is roughly the same as the in-sample error.
- statistics: unbiasedness.
- ML: robustness preferred over unbiasedness.
- neural networks are fit on very large data. regularization is typically used. be skeptical of complex models.
- noisy data. complex model performs worse.

- sample from the population. sampling distribution.
- complex model becomes unstable particularly in the part where data is sparse.
- high variance situation. high variance across random samples.
- K-fold CV. 30 data points. 5-fold 24 data points (more overfitting). over-estimate overfitting than overfitting using 30-data-point sample. more bias (worse performance) with respect to prediction performance

- iterator does not generate data. saves space
- sklearn only three things.
  - transforming. feature engineering. need to create polynomials etc
  - fitting
- sklearn expects list of lists.

- reshape
- columns features. rows observations.

- loading modeling function
- loading assessment function

\scriptsize
#+BEGIN_SRC python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
#+END_SRC
\normalsize

- create an estimator, fit on training data. state changes to a fitted model.
- training set error keeps decreasing with model complexity
- anreas pycon workshop for scikit learn

- training error goes down
- test error goes up and down. artifacts going up down.
- test set has been used at the point we chose.
- error associated with the hyperparameter (degree of polynomials)
- test set was used to choose d. not a good representation of out of sample error.
- test set could be bad.
- we have used up all the data for


* Validating Models
- validating
- regularization
- hyperparameters two in elastic net
- idiosyncratic validation set is solved with CV

** CV
- deterministic split bad if data is sorted.
- split has to be done up front.
- choose d based on cross validation.
- fit on the entire dataset at that d
- predict and report on held-out part.

- high variance with
- plot the errors individually. not just the mean.
- go back and refit the entire training test set.
- (train-validate)-test set


* Regularization

- model space restriction
- alpha is prior
- GridSearchCV
- sklearn.dataset
- score: R^{2} for regression or accuracy for classification


* Classification

- regression can be used
- K nearest neighbors
- what to do with probabilities
- scoring function in sklearn splits at 0.5
- asymmetry in scoring (classifying cancer based on probability)
